---
services:
  # For simplicity, we use Confluent Platform Zookeeper here. The procedure is exactly the same for another zookeeper image
  zookeeper1:
    image: confluentinc/cp-zookeeper:${CP_VERSION}
    hostname: zookeeper1
    container_name: zookeeper1
    profiles: 
      - zookeeper
      - migration
    restart: always
    ports:
      - "21811:21811"
      - "31801:31801"
      - "21801:21801"
    volumes:
      - data-zookeeper-log-1:/var/lib/zookeeper/log
      - data-zookeeper-data-1:/var/lib/zookeeper/data
    environment:
      ZOOKEEPER_TICK_TIME: 2000
      KAFKA_OPTS: "-Dzookeeper.4lw.commands.whitelist=*"
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_JMX_HOSTNAME: zookeeper1
      ZOOKEEPER_CLIENT_PORT: 21811
      ZOOKEEPER_JMX_PORT: 21801
      ZOOKEEPER_SERVERS: zookeeper1:2888:3888;zookeeper2:2888:3888;zookeeper3:2888:3888
 
  zookeeper2:
    image: confluentinc/cp-zookeeper:${CP_VERSION}
    hostname: zookeeper2
    container_name: zookeeper2
    profiles: 
      - zookeeper
      - migration
    restart: always
    ports:
      - "21812:21812"
      - "31802:31802"
      - "21802:21802"
    volumes:
      - data-zookeeper-log-2:/var/lib/zookeeper/log
      - data-zookeeper-data-2:/var/lib/zookeeper/data
    environment:
      ZOOKEEPER_TICK_TIME: 2000
      KAFKA_OPTS: "-Dzookeeper.4lw.commands.whitelist=*"
      ZOOKEEPER_SERVER_ID: 2
      ZOOKEEPER_JMX_HOSTNAME: zookeeper2
      ZOOKEEPER_CLIENT_PORT: 21812
      ZOOKEEPER_JMX_PORT: 21802
      ZOOKEEPER_SERVERS: zookeeper1:2888:3888;zookeeper2:2888:3888;zookeeper3:2888:3888

  zookeeper3:
    image: confluentinc/cp-zookeeper:${CP_VERSION}
    hostname: zookeeper3
    container_name: zookeeper3
    profiles: 
      - zookeeper
      - migration
    restart: always
    ports:
      - "21813:21813"
      - "31803:31803"
      - "21803:21803"
    volumes:
      - data-zookeeper-log-3:/var/lib/zookeeper/log
      - data-zookeeper-data-3:/var/lib/zookeeper/data
    environment:
      ZOOKEEPER_TICK_TIME: 2000
      KAFKA_OPTS: "-Dzookeeper.4lw.commands.whitelist=*"
      ZOOKEEPER_SERVER_ID: 3
      ZOOKEEPER_JMX_HOSTNAME: zookeeper3
      ZOOKEEPER_CLIENT_PORT: 21813
      ZOOKEEPER_JMX_PORT: 21803
      ZOOKEEPER_SERVERS: zookeeper1:2888:3888;zookeeper2:2888:3888;zookeeper3:2888:3888
 
  broker1:
    image: confluentinc/cp-server:${CP_VERSION}
    hostname: broker1
    container_name: broker1
    profiles: 
      - zookeeper
      - migration
      - kraft
    ports:
      - "29092:29092"
    volumes:
      - data-broker1:/var/lib/kafka/data
    environment:
      KAFKA_LISTENERS: BROKER://:9092,BROKER_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: BROKER://broker1:9092,BROKER_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: BROKER:PLAINTEXT,BROKER_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: true
      CONFLUENT_LINK_COORDINATOR_ENABLE: true
      KAFKA_CONFLUENT_CLUSTER_LINK_METADATA_TOPIC_ENABLE: true
      # Use these lines only during Phase 1 (Zookeeper mode) and Phase 2 (Migration): DO NOT USE these lines in Phase 3!
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:21811,zookeeper2:21812,zookeeper3:21813
      KAFKA_BROKER_ID: 1
      # Use these lines on in Phase 2 (Migration). DO NOT USE these lines in Phase 1 or Phase 3!
      #KAFKA_ZOOKEEPER_METADATA_MIGRATION_ENABLE: true
      # Use these lines on in Phase 2 (Migration) and Phase 3: Enable for migration and keep lines in final configuration
      #KAFKA_CONTROLLER_QUORUM_VOTERS: 101@controller1:9093,102@controller2:9093,103@controller3:9093
      #KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # Phase 3 (Migration finished): Enable these lines in the final configuration
      # Comment "KAFKA_BROKER_ID" (above), uncomment "KAFKA_NODE_ID" (below)
      #KAFKA_NODE_ID: 1
      #KAFKA_PROCESS_ROLES: broker
      #CLUSTER_ID: ${KRAFT_CLUSTER_ID}

  broker2:
    image: confluentinc/cp-server:${CP_VERSION}
    hostname: broker2
    container_name: broker2
    profiles: 
      - zookeeper
      - migration
      - kraft
    ports:
      - "29093:29093"
    volumes:
      - data-broker2:/var/lib/kafka/data
    environment:
      KAFKA_LISTENERS: BROKER://:9092,BROKER_HOST://0.0.0.0:29093
      KAFKA_ADVERTISED_LISTENERS: BROKER://broker2:9092,BROKER_HOST://localhost:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: BROKER:PLAINTEXT,BROKER_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: true
      KAFKA_CONFLUENT_CLUSTER_LINK_METADATA_TOPIC_ENABLE: true
      # Use these lines only during Phase 1 (Zookeeper mode) and Phase 2 (Migration): DO NOT USE these lines in Phase 3!
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:21811,zookeeper2:21812,zookeeper3:21813
      KAFKA_BROKER_ID: 2
      # Use these lines on in Phase 2 (Migration). DO NOT USE these lines in Phase 1 or Phase 3!
      #KAFKA_ZOOKEEPER_METADATA_MIGRATION_ENABLE: true
      # Use these lines on in Phase 2 (Migration) and Phase 3: Enable for migration and keep lines in final configuration
      #KAFKA_CONTROLLER_QUORUM_VOTERS: 101@controller1:9093,102@controller2:9093,103@controller3:9093
      #KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # Phase 3 (Migration finished): Enable these lines in the final configuration
      # Comment "KAFKA_BROKER_ID" (above), uncomment "KAFKA_NODE_ID" (below)
      #KAFKA_NODE_ID: 2
      #KAFKA_PROCESS_ROLES: broker
      #CLUSTER_ID: ${KRAFT_CLUSTER_ID}


  # KRaft Controllers
  controller1:
    image: confluentinc/cp-server:${CP_VERSION}
    hostname: controller1
    container_name: controller1
    profiles: 
      - migration
      - kraft
    volumes:
      - data-controller1:/var/lib/kafka/data
    environment:
      # Common
      KAFKA_PROCESS_ROLES: controller
      KAFKA_LISTENERS: CONTROLLER://controller1:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT
      KAFKA_NODE_ID: 101
      KAFKA_CONTROLLER_QUORUM_VOTERS: 101@controller1:9093,102@controller2:9093,103@controller3:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      CLUSTER_ID: ${KRAFT_CLUSTER_ID}
      KAFKA_KRAFT_REPLICATION_FACTOR: 3
      KAFKA_CONFLUENT_CLUSTER_LINK_METADATA_TOPIC_ENABLE: true
      # Phase 2 (Migration): Comment the next lines in Phase 3
      KAFKA_ZOOKEEPER_METADATA_MIGRATION_ENABLE: true
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:21811,zookeeper2:21812,zookeeper3:21813

  controller2:
    image: confluentinc/cp-server:${CP_VERSION}
    hostname: controller2
    container_name: controller2
    profiles: 
      - migration
      - kraft
    volumes:
      - data-controller2:/var/lib/kafka/data
    environment:
      # Common
      KAFKA_PROCESS_ROLES: controller
      KAFKA_LISTENERS: CONTROLLER://controller2:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT
      KAFKA_NODE_ID: 102
      KAFKA_CONTROLLER_QUORUM_VOTERS: 101@controller1:9093,102@controller2:9093,103@controller3:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      CLUSTER_ID: ${KRAFT_CLUSTER_ID}
      KAFKA_KRAFT_REPLICATION_FACTOR: 3
      KAFKA_CONFLUENT_CLUSTER_LINK_METADATA_TOPIC_ENABLE: true
      # Phase 2 (Migration): Comment the next lines in Phase 3
      KAFKA_ZOOKEEPER_METADATA_MIGRATION_ENABLE: true
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:21811,zookeeper2:21812,zookeeper3:21813

  controller3:
    image: confluentinc/cp-server:${CP_VERSION}
    hostname: controller3
    container_name: controller3
    profiles: 
      - migration
      - kraft
    volumes:
      - data-controller3:/var/lib/kafka/data
    environment:
      # Common
      KAFKA_PROCESS_ROLES: controller
      KAFKA_LISTENERS: CONTROLLER://controller3:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT
      KAFKA_NODE_ID: 103
      KAFKA_CONTROLLER_QUORUM_VOTERS: 101@controller1:9093,102@controller2:9093,103@controller3:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      CLUSTER_ID: ${KRAFT_CLUSTER_ID}
      KAFKA_KRAFT_REPLICATION_FACTOR: 3
      KAFKA_CONFLUENT_CLUSTER_LINK_METADATA_TOPIC_ENABLE: true
      # Phase 2 (Migration): Comment the next lines in Phase 3
      KAFKA_ZOOKEEPER_METADATA_MIGRATION_ENABLE: true
      KAFKA_ZOOKEEPER_CONNECT: zookeeper1:21811,zookeeper2:21812,zookeeper3:21813

  schema-registry:
    image: confluentinc/cp-schema-registry:${CP_VERSION}
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - broker1
      - broker2
    ports:
      - '8081:8081'
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker1:9092,broker2:9092'
      SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:8081"
      # TODO
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper1:21811,zookeeper2:21812,zookeeper3:21813

  control-center:
    image: confluentinc/cp-enterprise-control-center:${CP_VERSION}
    hostname: control-center
    container_name: control-center
    depends_on:
      - broker1
      - broker2
      - schema-registry
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker1:9092,broker2:9092'
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      PORT: 9021

  setup:
   image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
   profiles: [elastic]
   user: "0"
   command: >
     bash -c '
       if [ x${ELASTIC_PASSWORD} == x ]; then
         echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
         exit 1;
       elif [ x${KIBANA_PASSWORD} == x ]; then
         echo "Set the KIBANA_PASSWORD environment variable in the .env file";
         exit 1;
       fi;
       echo "Waiting for Elasticsearch availability";
       until curl -s http://es01:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
       echo "Setting kibana_system password";
       until curl -s -X POST -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" http://es01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
       echo "All done!";
     '

  es01:
   image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
   hostname: es01
   profiles: [elastic]
   labels:
     co.elastic.logs/module: elasticsearch
   volumes:
     - esdata01:/usr/share/elasticsearch/data
   ports:
     - ${ES_PORT}:9200
   environment:
     - node.name=es01
     - cluster.name=${CLUSTER_NAME}
     - discovery.type=single-node
     - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
     - bootstrap.memory_lock=true
     - xpack.security.enabled=true
     - xpack.security.http.ssl.enabled=false
     - xpack.security.transport.ssl.enabled=false
     - xpack.license.self_generated.type=${LICENSE}
   mem_limit: ${ES_MEM_LIMIT}
   ulimits:
     memlock:
       soft: -1
       hard: -1
   healthcheck:
     test:
       [
         "CMD-SHELL",
         "curl -s http://localhost:9200 | grep -q 'missing authentication credentials'",
       ]
     interval: 10s
     timeout: 10s
     retries: 120

  kibana:
   depends_on:
     es01:
       condition: service_healthy
   image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
   hostname: kibana
   profiles: [elastic]
   labels:
     co.elastic.logs/module: kibana
   volumes:
     - kibanadata:/usr/share/kibana/data
     - ./config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
   ports:
     - ${KIBANA_PORT}:5601
   environment:
     - SERVERNAME=kibana
     - ELASTICSEARCH_HOSTS=http://es01:9200
     - ELASTICSEARCH_USERNAME=kibana_system
     - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
     - XPACK_SECURITY_ENCRYPTIONKEY=${ENCRYPTION_KEY}
     - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${ENCRYPTION_KEY}
     - XPACK_REPORTING_ENCRYPTIONKEY=${ENCRYPTION_KEY}
     - XPACK_REPORTING_KIBANASERVER_HOSTNAME=localhost
     - SERVER_SSL_ENABLED=false
   mem_limit: ${KB_MEM_LIMIT}
   healthcheck:
     test:
       [
         "CMD-SHELL",
         "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
       ]
     interval: 10s
     timeout: 10s
     retries: 120

  # Docker container for running the configured fleet agent. The agent will capture docker logs
  fleet-server:
    depends_on:
      kibana:
        condition: service_healthy
      es01:
        condition: service_healthy
    profiles: [elastic]
    image: docker.elastic.co/beats/elastic-agent:${STACK_VERSION}
    volumes:
      - "fleetserverdata:/usr/share/elastic-agent"
      - "/var/lib/docker/containers:/var/lib/docker/containers:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "/sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro"
      - "/proc:/hostfs/proc:ro"
      - "/:/hostfs:ro"
    ports:
      - ${FLEET_PORT}:8220
    user: root
    environment:
      - FLEET_ENROLL=1
      - FLEET_INSECURE=true
      - FLEET_SERVER_ELASTICSEARCH_HOST=http://es01:9200
      - FLEET_SERVER_ELASTICSEARCH_INSECURE=true
      - FLEET_SERVER_ENABLE=1
      - FLEET_SERVER_INSECURE_HTTP=true
      - FLEET_SERVER_POLICY_ID=fleet-server-policy
      - FLEET_URL=http://fleet-server:8220
      - KIBANA_FLEET_SETUP=1
      - KIBANA_FLEET_USERNAME=elastic
      - KIBANA_FLEET_PASSWORD=${ELASTIC_PASSWORD}
      - KIBANA_HOST=http://kibana:5601

volumes:
  data-zookeeper-log-1:
    driver: local
  data-zookeeper-data-1:
    driver: local
  data-zookeeper-log-2:
    driver: local
  data-zookeeper-data-2:
    driver: local
  data-zookeeper-log-3:
    driver: local
  data-zookeeper-data-3:
    driver: local
  data-broker1:
    driver: local
  data-broker2:
    driver: local
  data-controller1:
    driver: local
  data-controller2:
    driver: local
  data-controller3:
    driver: local
  esdata01:
    driver: local
  kibanadata:
    driver: local
  fleetserverdata:
    driver: local
